# -*- coding: utf-8 -*-
"""BUAN6337_Project_Group6_(3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cGx-CKARKxc0jw5s6kWwpnsQXVn7R0xj

## Importing Libraries
"""

import warnings
warnings.filterwarnings(action='once')

#!pip install feature_engine
#!pip install --upgrade scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import plot_confusion_matrix, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, precision_recall_curve, auc, f1_score, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.tree import  export_graphviz
from sklearn.feature_selection import SelectFromModel
import graphviz

from feature_engine.encoding import OneHotEncoder
from feature_engine import transformation as vt
from feature_engine.wrappers import SklearnTransformerWrapper

import scipy.stats as st
import statsmodels.api as sm

from imblearn.over_sampling import SMOTE

"""## Importing Dataset"""

df = pd.read_csv("Customer_Churn.csv")

print(df.columns)
print(df.shape)

"""*This dataset contains data of 7043 customers and their attributes. We have demographics data of customers like gender, age range, senior citizen and if the customers have partners and dependents. We also have data about customer's association with the company, like tenure, if they use phone service and have multiple lines, if the customer uses services like Internet Service, Online security, Online Backup, Device Protection, Tech Supprt, Streaming TV and Movies. Finance related information includes type of contract, if the customer has opted for Paperless Billing, their payment method, monthly charges and total charges. The class/target variable is this dataset is Churn which classifies customers as yes if they are no more associated with the company.<br>
We have some very useful attributes about customers to predict customer churn. For example,<br>
Monthly charges - If the monthly charges are very high, customer might want to switch to another provider.<br>
Contract - If the customers are on short term contract, they are more likely to switch to another provider. If they are long-term contract they would tend to stay with the provider.<br>*

## Exploratory Data Analysis

### Taking a sample of 5 observations
"""

df.sample(5)

df.shape

df['Churn'].hist()
plt.show()

"""*We can see there is an class imbalance in the dependent variable.*

### Preprocessing

*For our analysis we will not require customerID column. Dropping the column.*<br>
"""

df.drop(columns=['customerID'], inplace=True)

df.dtypes

for var in df.columns:
    print(f"\nNumber of unique values in {var}: {df[var].nunique()}")
    print(df[var].unique())

"""*We will change values of some columns to better readability and some values seems redundant like in TechSupport No internet service actually means customer has not availed Tech Support. Since we already have observations with No in Internet Service Column, we will replace these values with No.*"""

df.replace({'Month-to-month':'Monthly', 
            'Electronic check':'Electronic',
            'Mailed check':'Mailed',
            'Bank transfer (automatic)':'Bank transfer',
            'Credit card (automatic)':'Credit card', 
            'No internet service': 'No',
            'No phone service': 'No'
           }, inplace=True)

"""*We can see that SeniorCitizen is of type* ***int***, *we will change the data type to* ***category.***<br>
*Total charges is of type* ***object***, *we will change the data type to* ***float*** *by converting it to numeric values*.<br>
*We will also change all the columns with* ***object*** *data type to* ***category*** *data type*<br>
*There doesn't seem to be any variable which is of* ***ordinal*** *type*
"""

df['SeniorCitizen'] = df['SeniorCitizen'].astype('category')

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

objCols = [var for var in df.columns if df[var].dtype == 'object']
objCols

for cols in objCols:
    df[cols] = df[cols].astype('category')
df.dtypes

"""### Divide variables into quantitative and qualitative"""

qual_var = [var for var in df.columns if df[var].dtype.name == 'category' and var != 'Churn']
quan_var = [var for var in df.columns if df[var].dtype.name != 'category']
print(qual_var)
print(quan_var)

"""### Check for NA/NULL Values"""

print(df.isnull().sum())

from IPython.display import Markdown as md
md(f"*We have {df['TotalCharges'].isnull().sum()} observations with NULL values which is {round(df['TotalCharges'].isnull().sum()/df.shape[0]*100,2)}% of all the observations, we can drop these rows as dropping them will not affect our analysis.*")

"""### Drop observations with NA values"""

df = df.dropna()

df.describe()

"""*Mean tenure is 32 months, mean monthly charges is \\$65 and mean total charges for each customer is \\$2283 which is marginally higher than average monthly times average tenure.*"""

def chiSqTest(var1,var2,a):
    chi2, p, degf, expected = st.chi2_contingency(pd.crosstab(var1,var2))
    print('Observed\n')
    print(pd.crosstab(var1,var2))
    print('Expected\n')
    print(expected)
    print(f'chi^2 = {chi2:.4f}\n')
    print(f'p     = {p:.4f}\n')
    if p < a:
        print('P is less than alpha. We reject the null hypothesis.\n')
    else:
        print('P is more than alpha. We cannot reject the null hypothesis.')

for var in qual_var:
    print(f"\nDoes {var} reduce churn?\nNull hypothesis: Churn is independent from {var}\nAlternative hypothesis: churn is dependent on {var}")
    chiSqTest(df[var],df['Churn'],0.05)

def tTests(var1,var2,a):
    a = 0.05
    print(f'Var1 = {var1.var():.3f}\n')
    print(f'Var2 = {var2.var():.3f}\n')
    stat, p = st.ttest_ind(var1,var2)
    p *= 2
    print(f'statistic = {stat:.3f}\n')
    print(f'p_value = {p:.3f}\n')
    if p < a:
        print('P is less than alpha. We reject the null hypothesis.\n')
    if p > a:
        print('P is more than alpha. We fail to reject the null hypothesis.\n')

for var in quan_var:
    var1 = df[df['Churn']=="Yes"][var]
    var2 = df[df['Churn']=="No"][var]
    print(f"\nDoes {var} has any effect on churn?\nNull hypothesis: Churn is independent from {var}\nAlternative hypothesis: churn is dependent on {var}")
    tTests(var1,var2,0.05)

def diagnostic_plots(df, variable):
    plt.figure(figsize=(16, 3))
    plt.subplot(1, 3, 1)
    sns.histplot(df[variable], bins=30)
    plt.title('Histogram')
    plt.subplot(1, 3, 2)
    sns.boxplot(y=df[variable])
    plt.title('Boxplot')
    plt.subplot(1, 3, 3)
    st.probplot(df[variable], dist="norm", plot=plt)
    plt.title('Q-Q plot')
    plt.show()

for var in quan_var:
    diagnostic_plots(df,var)

for var in qual_var:
    print(f"Count plot for {var}")
    ax = sns.countplot(x=df[var], hue=df['Churn'])
    plt.show()

"""### Observations
* *Gender distribution shows there relatively equal number of Male and Female who stayed with the provider and who choose to discontinue the service*<br>
* *Most of the customers are not Senior Citizens*<br>
* *Customers who have partners are marginally higher than customers who don\'t. And customer who have dependents are relatively lower than customers who don\'t.*<br>
* *Most of the customers uses Phone Service, does not have multiple lines, uses either DSL or Fiber optics, and have internet services*<br>
* *While most customers are on monthly plans, there\'s almost equal distribution of customers on long term plans.*<br>
* *Most customers prefer paperless billing and uses Electronic mode of payment. Also, the customers who churned are most in this segment.*<br>

### Correlation between independent variables
"""

corr = df.drop(columns=['Churn']).apply(lambda x : pd.factorize(x)[0]).corr(method='pearson')
plt.figure(figsize=(15,6))
cmap = sns.diverging_palette(255, 0, as_cmap=True)
sns.heatmap(corr, square=True, cmap=cmap)
plt.show()

"""*We can see that none of the independent variables have a very high correlation. StreamingTV and StreamMovies have maximum correlation of 0.53.*

### Correlation between quantitative variables
"""

corr_quan = df[quan_var].corr()
plt.figure(figsize=(15,6))
cmap = sns.diverging_palette(255, 0, as_cmap=True)
sns.heatmap(corr_quan, square=True, cmap=cmap)
plt.show()

sns.scatterplot(x= df['tenure'], y=df['TotalCharges'])
plt.show()

"""### Check for outliers in quantitative variables"""

df[quan_var].describe()

Q1 = df[quan_var].quantile(0.25)
Q3 = df[quan_var].quantile(0.75)
IQR = Q3 - Q1
((df[quan_var] < Q1 - (1.5 * IQR)) | (df[quan_var] > Q3 + (1.5 * IQR))).any()

df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

"""### Learning Curve Function"""

def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,
                        n_jobs=None, train_sizes=np.linspace(.2, 1.0, 5)):
    if axes is None:
        _, axes = plt.subplots(1, 2, figsize=(10, 5))

    axes[0].set_title(title)
    if ylim is not None:
        axes[0].set_ylim(*ylim)
    axes[0].set_xlabel("Training examples")
    axes[0].set_ylabel("Score")

    train_sizes, train_scores, test_scores, fit_times, _ = \
        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,
                       train_sizes=train_sizes,
                       return_times=True,
                       random_state=123)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    fit_times_mean = np.mean(fit_times, axis=1)
    fit_times_std = np.std(fit_times, axis=1)

    # Plot learning curve
    axes[0].grid()
    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,
                         train_scores_mean + train_scores_std, alpha=0.1,
                         color="r")
    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,
                         test_scores_mean + test_scores_std, alpha=0.1,
                         color="g")
    axes[0].plot(train_sizes, train_scores_mean, 'o-', color="r",
                 label="Training score")
    axes[0].plot(train_sizes, test_scores_mean, 'o-', color="g",
                 label="Cross-validation score")
    axes[0].legend(loc="best")

    # Plot n_samples vs fit_times
    axes[1].grid()
    axes[1].plot(train_sizes, fit_times_mean, 'o-')
    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,
                         fit_times_mean + fit_times_std, alpha=0.1)
    axes[1].set_xlabel("Training examples")
    axes[1].set_ylabel("fit_times")
    axes[1].set_title("Scalability of the model")

    return plt

"""### Functions for metrics"""

def confusion_matrix_plot(X_test, y_test, classifier, classifier_name):
    fig, ax = plt.subplots(figsize=(7, 6))
    plot_confusion_matrix(classifier, X_test, y_test, display_labels=["No Churn", "Churn"], cmap=plt.cm.Blues, normalize=None, ax=ax)
    ax.set_title(f'{classifier_name} - Confusion Matrix')
    plt.show()
    fig, ax = plt.subplots(figsize=(7, 6))
    plot_confusion_matrix(classifier, X_test, y_test, display_labels=["No Churn", "Churn"], cmap=plt.cm.Blues, normalize='true', ax=ax)
    ax.set_title(f'{classifier_name} - Confusion Matrix (norm.)')
    plt.show()
def roc_curve_plot(X_test, y_test, classifier, classifier_name):
    y_pred_prob = classifier.predict_proba(X_test)[:,1]
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.plot(fpr, tpr, label=f'{classifier_name}')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'{classifier_name} - ROC Curve')
    plt.show()
def metrics(X_train, y_train, X_test, y_test, y_pred, y_prob, classifier, classifier_name):
    print(f'{classifier_name} Metrics')
    acc_train = classifier.score(X_train, y_train)
    acc_test = classifier.score(X_test, y_test)
    pre_test = precision_score(y_test, y_pred)
    rec_test = recall_score(y_test, y_pred)
    roc_test = roc_auc_score(y_test, y_prob[:, 1])
    f1_test = f1_score(y_test, y_pred)
    precision, recall, thresholds = precision_recall_curve(y_test, y_prob[:, 1])
    auc_test = auc(recall, precision)
    print(f'Accuracy (Train): {acc_train}')
    print(f'Accuracy (Test): {acc_test}')
    print(f'Precision (Test): {pre_test}')
    print(f'Recall (Test): {rec_test}')
    print(f'ROC AUC Score (Test): {roc_test}')
    print(f'F1 Score (Test): {f1_test}')
    print(f'AUC Score (Test): {auc_test}')
    out = [acc_train, acc_test, pre_test, rec_test, roc_test, f1_test, auc_test]
    return out
def precision_recall_curve_and_scores(X_test, y_test, y_pred, y_prob, classifier_name):
    precision, recall, thresholds = precision_recall_curve(y_test, y_prob[:, 1])
    plt.plot(recall, precision, label=f'{classifier_name}')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'{classifier_name} - Precision-Recall Curve')
    plt.show()
def feature_weights(X_df, classifier, classifier_name):
    weights = pd.Series(classifier.best_estimator_.coef_[0], index=X_df.columns.values).sort_values(ascending=False)
    
    top_weights_selected = weights[:10]
    plt.figure(figsize=(7,6))
    plt.tick_params(labelsize=10)#plt.xlabel(fontsize=10)
    plt.title(f'{classifier_name} - Top 10 Features')
    top_weights_selected.plot(kind="bar")
    
    bottom_weights_selected = weights[-10:]
    plt.figure(figsize=(7,6))
    plt.tick_params(labelsize=10)#plt.xlabel(fontsize=10)
    plt.title(f'{classifier_name} - Bottom 10 Features')
    bottom_weights_selected.plot(kind="bar")
    
    return print("")

"""### Train Test Split"""

X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Churn']), df['Churn'], test_size=0.2, random_state=100, stratify = df['Churn'])

"""#### Preprocessing"""

encoder = OneHotEncoder(drop_last=True, variables=qual_var)
tf = vt.YeoJohnsonTransformer(variables=quan_var)
scaler = SklearnTransformerWrapper(transformer=MinMaxScaler(), variables=quan_var)

encoder.fit(X_train)
X_train = encoder.transform(X_train)
X_test = encoder.transform(X_test)

tf.fit(X_train)
X_train = tf.transform(X_train)
X_test = tf.transform(X_test)

scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""### Logit Model"""

X_train_c = sm.add_constant(X_train)
logit_model = sm.Logit(y_train,X_train_c)
result = logit_model.fit()
print(result.summary())

"""8 iterations has been done to find the optimized model<br>
coef : the coefficients of the independent variables in the regression equation.<br>
Log-Likelihood : the natural logarithm of the Maximum Likelihood Estimation(MLE) function. MLE is the optimization process of finding the set of parameters that result in the best fit.<br>
LL-Null : the value of log-likelihood of the model when no independent variable is included(only an intercept is included).<br>
Pseudo R-squ. : a substitute for the R-squared value in Least Squares linear regression. It is the ratio of the log-likelihood of the null model to that of the full model.<br>
"""

prediction = list(map(round, result.predict(sm.add_constant(X_train))))
ConfusionMatrixDisplay(confusion_matrix(y_train, prediction), display_labels=['No','Yes']).plot()
plt.show()
print(f'Accuracy: {accuracy_score(y_train, prediction)}')
print(f'Precision: {precision_score(y_train, prediction)}')
print(f'Recall: {recall_score(y_train, prediction)}')
print(f'F1 Score: {f1_score(y_train, prediction)}')
print(f'ROC AUC: {roc_auc_score(y_train, prediction)}')

prediction = list(map(round, result.predict(sm.add_constant(X_test))))
ConfusionMatrixDisplay(confusion_matrix(y_test, prediction), display_labels=['No','Yes']).plot()
plt.show()
print(f'Accuracy: {accuracy_score(y_test, prediction)}')
print(f'Precision: {precision_score(y_test, prediction)}')
print(f'Recall: {recall_score(y_test, prediction)}')
print(f'F1 Score: {f1_score(y_test, prediction)}')
print(f'ROC AUC: {roc_auc_score(y_test, prediction)}')

np.unique(y_train, return_counts=True)

smote = SMOTE(random_state=123)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

np.unique(y_train_res, return_counts=True)

"""### Logit after oversampling"""

X_train_res_c = sm.add_constant(X_train_res)
logit_model_os = sm.Logit(y_train_res,X_train_res_c)
result1 = logit_model_os.fit()
print(result1.summary())

prediction = list(map(round, result1.predict(sm.add_constant(X_train_res))))
#print(prediction)
ConfusionMatrixDisplay(confusion_matrix(y_train_res, prediction), display_labels=['No','Yes']).plot()
plt.show()
print(f'Accuracy: {accuracy_score(y_train_res, prediction)}')
print(f'Precision: {precision_score(y_train_res, prediction)}')
print(f'Recall: {recall_score(y_train_res, prediction)}')
print(f'F1 Score: {f1_score(y_train_res, prediction)}')
print(f'ROC AUC: {roc_auc_score(y_train_res, prediction)}')

prediction = list(map(round, result1.predict(sm.add_constant(X_test))))
#print(prediction)
ConfusionMatrixDisplay(confusion_matrix(y_test, prediction), display_labels=['No','Yes']).plot()
plt.show()
print(f'Accuracy: {accuracy_score(y_test, prediction)}')
print(f'Precision: {precision_score(y_test, prediction)}')
print(f'Recall: {recall_score(y_test, prediction)}')
print(f'F1 Score: {f1_score(y_test, prediction)}')
print(f'ROC AUC: {roc_auc_score(y_test, prediction)}')

"""#### KNN Classifier"""

knn = KNeighborsClassifier()
knn.fit(X_train_res, y_train_res)
y_pred_knn = knn.predict(X_test)
y_pred_knn_prob = knn.predict_proba(X_test)

warnings.filterwarnings('ignore')
confusion_matrix_plot(X_test, y_test, knn, 'KNN')
roc_curve_plot(X_test, y_test, knn, 'KNN')
metrics(X_train_res, y_train_res, X_test, y_test, y_pred_knn, y_pred_knn_prob, knn, 'KNN')
precision_recall_curve_and_scores(X_test, y_test, y_pred_knn, y_pred_knn_prob, 'KNN')

"""#### Logistic Regression"""

warnings.filterwarnings('ignore')
logreg = LogisticRegression()
logreg.fit(X_train_res, y_train_res)
y_pred_logreg = logreg.predict(X_test)
y_pred_logreg_prob = logreg.predict_proba(X_test)

warnings.filterwarnings('ignore')
confusion_matrix_plot(X_test, y_test, logreg, 'Logistic')
roc_curve_plot(X_test, y_test, logreg, 'Logistic')
metrics(X_train_res, y_train_res, X_test, y_test, y_pred_logreg, y_pred_logreg_prob, logreg, 'Logistic')
precision_recall_curve_and_scores(X_test, y_test, y_pred_logreg, y_pred_logreg_prob, 'Logistic')

coeff = pd.DataFrame(np.vstack([X_train_res.columns,logreg.coef_]).T,columns=['Variables','Coefficients'])
intercept = pd.DataFrame(np.vstack(["Intercept",logreg.intercept_]).T,columns=['Variables','Coefficients'])
pd.concat([intercept,coeff],axis=0).reset_index().drop(columns=['index'])

"""#### Decision Tree"""

dt = DecisionTreeClassifier()
dt.fit(X_train_res, y_train_res)
y_pred_dt = dt.predict(X_test)
y_pred_dt_prob = dt.predict_proba(X_test)

warnings.filterwarnings('ignore')
confusion_matrix_plot(X_test, y_test, dt, 'Decision Tree')
roc_curve_plot(X_test, y_test, dt, 'Decision Tree')
metrics(X_train_res, y_train_res, X_test, y_test, y_pred_dt, y_pred_dt_prob, dt, 'Decision Tree')
precision_recall_curve_and_scores(X_test, y_test, y_pred_dt, y_pred_dt_prob, 'Decision Tree')

"""#### Random Forest"""

rf = RandomForestClassifier()
rf.fit(X_train_res, y_train_res)
y_pred_rf = rf.predict(X_test)
y_pred_rf_prob = rf.predict_proba(X_test)

warnings.filterwarnings('ignore')
confusion_matrix_plot(X_test, y_test, rf, 'Random Forest')
roc_curve_plot(X_test, y_test, rf, 'Random Forest')
metrics(X_train_res, y_train_res, X_test, y_test, y_pred_rf, y_pred_rf_prob, rf, 'Random Forest')
precision_recall_curve_and_scores(X_test, y_test, y_pred_rf, y_pred_rf_prob, 'Random Forest')

"""### Hyperparameter tuning

#### KNN Classifier
"""

param_grid_knn = {"n_neighbors": np.arange(1,30)}
knn = KNeighborsClassifier()
knn = GridSearchCV(knn, param_grid_knn, cv = 5, return_train_score=True)
#knn = RandomizedSearchCV(knn, param_grid_knn, cv = 5, return_train_score=True, n_jobs=-1, random_state=123, n_iter= 100)
knn.fit(X_train_res, y_train_res)
y_pred_knn = knn.predict(X_test)
y_pred_knn_prob = knn.predict_proba(X_test)

warnings.filterwarnings('ignore')
print(f'Best Estimator: {knn.best_estimator_}')
confusion_matrix_plot(X_test, y_test, knn, 'KNN')
roc_curve_plot(X_test, y_test, knn, 'KNN')
metrics(X_train_res, y_train_res, X_test, y_test, y_pred_knn, y_pred_knn_prob, knn, 'KNN')
precision_recall_curve_and_scores(X_test, y_test, y_pred_knn, y_pred_knn_prob, 'KNN')
plot_learning_curve(knn.best_estimator_,'Learning Curve KNN',X_train_res,y_train_res,n_jobs=-1)

"""#### Logistic Regression"""

warnings.filterwarnings('ignore')
param_grid_logreg = {"penalty": ['l1', 'l2'], 
                     "C": np.arange(.1, 5, .1), 
                     "max_iter": np.logspace(0,3,4)}
logreg = LogisticRegression(random_state=123)
logreg = GridSearchCV(logreg, param_grid_logreg, cv = 5, return_train_score=True)
#logreg = RandomizedSearchCV(logreg, param_grid_logreg, cv = 5, return_train_score=True, n_jobs=-1, random_state=123, n_iter= 100)
logreg.fit(X_train_res, y_train_res)
y_pred_logreg = logreg.predict(X_test)
y_pred_logreg_prob = logreg.predict_proba(X_test)

print(f'Best Estimator: {logreg.best_estimator_}')
confusion_matrix_plot(X_test, y_test, logreg, 'Logistic')
roc_curve_plot(X_test, y_test, logreg, 'Logistic')
metrics(X_train_res, y_train_res, X_test, y_test, y_pred_logreg, y_pred_logreg_prob, logreg, 'Logistic')
precision_recall_curve_and_scores(X_test, y_test, y_pred_logreg, y_pred_logreg_prob, 'Logistic')
plot_learning_curve(logreg.best_estimator_,'Learning Curve Logistic Regression',X_train_res,y_train_res,n_jobs=-1)
feature_weights(X_train_res, logreg, 'Log. Regression')

coeff = pd.DataFrame(np.vstack([X_train_res.columns,logreg.best_estimator_.coef_]).T,columns=['Variables','Coefficients'])
intercept = pd.DataFrame(np.vstack(["Intercept",logreg.best_estimator_.intercept_]).T,columns=['Variables','Coefficients'])
pd.concat([intercept,coeff],axis=0).reset_index().drop(columns=['index'])

"""#### Decision Tree"""

warnings.filterwarnings('ignore')
param_grid_dt = {"max_depth": np.arange(2,40,2), 
                 "min_samples_leaf": np.arange(2,40,2), 
                 "min_samples_split": np.arange(2,40,2), 
                 "max_leaf_nodes": np.arange(2,40,2),
                 "min_impurity_decrease": np.logspace(-5,0,10)}
dt = DecisionTreeClassifier(random_state=123)
#dt = GridSearchCV(dt, param_grid_dt, cv = 5, return_train_score=True)
dt = RandomizedSearchCV(dt, param_grid_dt, cv = 5, return_train_score=True, n_jobs=-1, random_state=123, n_iter= 100)
dt.fit(X_train_res, y_train_res)
y_pred_dt = dt.predict(X_test)
y_pred_dt_prob = dt.predict_proba(X_test)

print(f'Best Estimator: {dt.best_estimator_}')
confusion_matrix_plot(X_test, y_test, dt, 'Decision Tree')
roc_curve_plot(X_test, y_test, dt, 'Decision Tree')
metrics(X_train_res, y_train_res, X_test, y_test, y_pred_dt, y_pred_dt_prob, dt, 'Decision Tree')
precision_recall_curve_and_scores(X_test, y_test, y_pred_dt, y_pred_dt_prob, 'Decision Tree')
plot_learning_curve(dt.best_estimator_,'Learning Curve Decision Tree',X_train_res,y_train_res,n_jobs=-1)

tree_dot = export_graphviz(dt.best_estimator_, out_file=None, feature_names=X_train_res.columns, filled=True)
graph = graphviz.Source(tree_dot, format="png")
graph

"""#### Random Forest"""

warnings.filterwarnings('ignore')
param_grid_rf = {'max_depth':np.arange(2,40,2),
                 'n_estimators': np.arange(10,200,10),
                 'min_samples_leaf': np.arange(2,15,2),
                 'max_leaf_nodes': np.arange(5,50,5)}
rf = RandomForestClassifier(random_state=123)
#rf = GridSearchCV(rf, param_grid_rf, cv = 5, return_train_score=True)
rf = RandomizedSearchCV(rf, param_grid_rf, cv = 5, return_train_score=True, n_jobs=-1, random_state=123, n_iter= 100)
rf.fit(X_train_res, y_train_res)
y_pred_rf = rf.predict(X_test)
y_pred_rf_prob = rf.predict_proba(X_test)

print(f'Best Estimator: {rf.best_estimator_}')
confusion_matrix_plot(X_test, y_test, rf, 'Random Forest')
roc_curve_plot(X_test, y_test, rf, 'Random Forest')
metrics(X_train_res, y_train_res, X_test, y_test, y_pred_rf, y_pred_rf_prob, rf, 'Random Forest')
precision_recall_curve_and_scores(X_test, y_test, y_pred_rf, y_pred_rf_prob, 'Random Forest')
plot_learning_curve(rf.best_estimator_,'Learning Curve Random Forest',X_train_res,y_train_res,n_jobs=-1)

"""### Model Comparison"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# knn_metric = metrics(X_train_res, y_train_res, X_test, y_test, y_pred_knn, y_pred_knn_prob, knn, 'KNN')
# logreg_metric = metrics(X_train_res, y_train_res, X_test, y_test, y_pred_logreg, y_pred_logreg_prob, logreg, 'Logistic')
# dt_metric = metrics(X_train_res, y_train_res, X_test, y_test, y_pred_dt, y_pred_dt_prob, dt, 'Decision Tree')
# rf_metric = metrics(X_train_res, y_train_res, X_test, y_test, y_pred_rf, y_pred_rf_prob, rf, 'Random Forest')

metric_table = pd.DataFrame({'KNN':knn_metric,
                             'Logistic Regression':logreg_metric,
                             'Decision Tree':dt_metric,
                             'Random Forest':rf_metric}, index=['Accuracy (Train)', 
                                                                'Accuracy (Test)', 
                                                                'Precision (Test)', 
                                                                'Recall (Test)', 
                                                                'ROC AUC Score (Test)', 
                                                                'F1 Score (Test)', 
                                                                'AUC Score (Test)'])

metric_table

final_model = RandomForestClassifier(max_depth=28, 
                                       max_leaf_nodes=45, 
                                       min_samples_leaf=4, 
                                       n_estimators=30, 
                                       random_state=123)
final_model.fit(X_train_res,y_train_res)
feat_labels = X_train_res.columns

feature_list = {}
for feature,importance in zip(X_train_res.columns, final_model.feature_importances_):
    feature_list[feature] = importance
print(dict(sorted(feature_list.items(), key=lambda item: item[1],reverse=True)))
sfm = SelectFromModel(final_model, threshold=0.10)
sfm.fit(X_train_res, y_train_res)
for feature_list_index in sfm.get_support(indices=True):
    print(feat_labels[feature_list_index])
    
sns.lineplot(x=feature_list.keys(), y=feature_list.values(), data=feature_list)
plt.xticks(rotation=90)
plt.show()

feature_list.keys()

"""# Confusion matrix threshold tuning"""

def plot_pr_curve(p, r, thresholds, model_type, dt=[0.5]):

    # plot the curve
    plt.figure(figsize=(8,8))
    plt.title(f'{model_type} | Precision-Recall Curve | Shows Decision Thresholds')
    plt.step(r, p, color='b', alpha=0.2, where='post')
    plt.fill_between(r, p, step='post', alpha=0.2, color='b')
    plt.ylim([0, 1.01])
    plt.xlim([0, 1.01])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    
    # plot the current threshold on the line
    for t in dt:
        dt_point = np.argmin(np.abs(thresholds - t))
        dt_label = 'DT = '+str(t)
        plt.plot(r[dt_point], p[dt_point], 'o', c='k', markersize=12)
        plt.annotate(dt_label, 
                     (r[dt_point], p[dt_point]),
                     textcoords="offset points", 
                     xytext=(0,12), 
                     ha='center')

# generate Logistic Regression predict probabilities; store precision-recall curve values
y_pred_logreg_prob_2 = logreg.predict_proba(X_test)[:, 1]
p2, r2, thresholds2 = precision_recall_curve(y_test, y_pred_logreg_prob_2)

# plot precision-recall curve for baseline decision threshold 0.5
plot_pr_curve(p2, r2, thresholds2, 'Logistic Regression', dt=[0.5])

def plot_pr_vs_dt(p, r, thresholds, model_type):
    """
    Plots precision and recall lines (y) for each value of classifier decision
    threshold (x). Default classifier models use >= 0.5.
   
    Parameters:
    - p (array): precision value at certain threshold
    - r (array): recall value at certain threshold
    - thresholds (array): probability threshold where positive class is assigned
    - model_type (str): Text for model type title

    Returns: None
    """
    plt.figure(figsize=(8, 8))
    plt.title(f'{model_type} | Precision-Recall Scores vs. Decision Threshold')
    plt.plot(thresholds, p[:-1], 'b--', label='Precision')
    plt.plot(thresholds, r[:-1], 'g-', label='Recall')
    plt.ylabel('Score')
    plt.xlabel('Decision Threshold')
    plt.legend(loc='best')

# plot precision and recall against decision threshold options
plot_pr_vs_dt(p2, r2, thresholds2, 'Logistic Regression')

def assign_class(y_hat_prob, dt=0.5):
    """
    Calculates churn predictions based on any decision threshold level 
    using a classification model's predicted probability scores.
    
    Parameters:
    - y_hat_prob (array): predicted churn probabilities
    - dt (float): decision threshold value between 0 and 1. Represents 
      predicted probability at which postive class (churn) should be assigned.

    Returns:
    - scores (dict): includes 5 evaluation metrics for model type
    """
    return [1 if y >= dt else 0 for y in y_hat_prob]

# plot confusion matrix for classifier object
def class1_confusion_matrix_plot(y_true, y_hat):
    """Prints classification report for a classifier model predictions.

    Parameters:
    - y_true (pd.Series): true north y values (actual churn values)
    - y_hat (array): model predictions for y    

    Returns: none
    """
    print(pd.DataFrame(confusion_matrix(y_true, y_hat),
                       columns=['Retain Pred', 'Churn Pred'], 
                       index=['Retain Actual', 'Churn Actual']))
    print(classification_report(y_true, y_hat))

# revised class predictions using decision threshold 0.5
y_pred_low_2 = assign_class(y_pred_logreg_prob_2, dt=0.5)
# confusion_matrix(y_test, y_pred_adj)
class1_confusion_matrix_plot(y_test, y_pred_low_2)

# revised class predictions using decision threshold 0.43
y_pred_med_2 = assign_class(y_pred_logreg_prob_2, dt=0.35)
# confusion_matrix(y_test, y_pred_adj)
class1_confusion_matrix_plot(y_test, y_pred_med_2)

# revised class predictions using decision threshold 0.73
y_pred_hi_2 = assign_class(y_pred_logreg_prob_2, dt=0.23)
# confusion_matrix
class1_confusion_matrix_plot(y_test, y_pred_hi_2)



